üìå Step 1: Set Up the Project Locally
We'll create three files:

1. app.py - FastAPI-based ML API
2. requirements.txt - Python dependencies
3. Dockerfile - To build a Docker image


1Ô∏è‚É£ File: app.py (FastAPI ML API)
code >>>>>>>>>>>>>>>>>>

from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

app = FastAPI()

# Load and train the model
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=10, random_state=42)
model.fit(X_train, y_train)

# Define request body
class InputData(BaseModel):
    features: list[float]

@app.get("/")
def home():
    return {"message": "Iris Classification API is running!"}

@app.post("/predict")
def predict(data: InputData):
    try:
        features = np.array(data.features).reshape(1, -1)
        prediction = model.predict(features)
        return {"prediction": int(prediction[0])}
    except Exception as e:
        return {"error": str(e)}


2Ô∏è‚É£ File: requirements.txt

fastapi
uvicorn
numpy
scikit-learn


3Ô∏è‚É£ File: Dockerfile

# Use an official Python runtime as a parent image
FROM python:3.9

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container
COPY . /app

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Expose the port
EXPOSE 8000

# Run the FastAPI app using Uvicorn
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

üìå Step 2: Run Docker Locally
Install Docker (if not installed)
Download & Install Docker

Build the Docker Image

docker build -t fastapi-iris .
Run the Docker Container

docker run -p 8000:8000 fastapi-iris
Test the API
Open in browser:

arduino
---------
http://127.0.0.1:8000/docs
Or test with curl:

curl -X POST "http://127.0.0.1:8000/predict" -H "Content-Type: application/json" -d '{"features": [5.1, 3.5, 1.4, 0.2]}'
Expected response:

{"prediction": 0}
üìå Step 3: Deploy to AWS EC2
1Ô∏è‚É£ Launch EC2 Instance
Open AWS Console ‚Üí EC2 ‚Üí Launch Instance
Choose Ubuntu 22.04 or Amazon Linux 2
Instance type: t3.micro
Enable HTTP/HTTPS ports in Security Groups
Key Pair: Create or use an existing key pair
2Ô∏è‚É£ Connect to EC2
From your local machine:
ssh -i your-key.pem ubuntu@your-ec2-public-ip
3Ô∏è‚É£ Install Docker on EC2
sudo apt update
sudo apt install -y docker.io
Verify installation:

docker --version
4Ô∏è‚É£ Transfer Project to EC2
On your local machine:

scp -i your-key.pem -r fastapi-iris ubuntu@your-ec2-public-ip:~

Then SSH into the EC2 instance:

ssh -i your-key.pem ubuntu@your-ec2-public-ip

ssh -i classification.pem ubuntu@13.51.206.229
cd fastapi-iris

5Ô∏è‚É£ Build & Run Docker on EC2

docker build -t fastapi-iris .
docker run -d -p 8000:8000 fastapi-iris
6Ô∏è‚É£ Test the API on EC2

curl -X POST "http://your-ec2-public-ip:8000/predict" -H "Content-Type: application/json" -d '{"features": [5.1, 3.5, 1.4, 0.2]}'
Or open:
sudo docker logs c2cda0ac878eea16262ca0f75f5806ed0ec9a2fd60e665beb1aba5ed3bd56867

check this page
http://13.51.206.229:8000/

or 
curl -X POST "http://13.51.206.229:8000/predict" -H "Content-Type: application/json" -d '{"features": [5.1, 3.5, 1.4, 0.2]}'

http://your-ec2-public-ip:8000/docs
üéØ Summary
‚úÖ Built a FastAPI ML model locally
‚úÖ Containerized the app using Docker
‚úÖ Deployed on AWS EC2

üöÄ You now have a working ML API on EC2! üéâ